# nexera-aiai-3D-Avatar-training-project
Whatâ€™s Included âœ… Prototype 1 â€” AI â†’ 3D Learning Asset

User enters a text description (e.g. â€œyellow hard hatâ€)

AI generates an educational explanation

A relevant 3D model (GLB) is selected and displayed

Model is auto-scaled, positioned, and interactive

Suitable for training and learning modules

âœ… Prototype 2 â€” Natural Language â†’ Avatar Animation

User types commands like:

â€œWave hello to the learnerâ€

â€œShow the correct safety postureâ€

AI interprets the command into structured intent

Avatar plays the corresponding animation

AI provides a short explanation of the action

Tech Stack Frontend

Babylon.js

Vanilla JavaScript

HTML / CSS

Backend

Python

FastAPI

Uvicorn

AI

Ollama (local LLM)

Project Structure nexera-ai/ â”œâ”€â”€ backend/ â”‚ â”œâ”€â”€ main.py â”‚ â”œâ”€â”€ ollama_client.py â”‚ â”œâ”€â”€ schemas.py â”‚ â”œâ”€â”€ assets/ â”‚ â”‚ â””â”€â”€ models/ â”‚ â”‚ â”œâ”€â”€ hard_hat.glb â”‚ â”‚ â””â”€â”€ generic_object.glb â”‚ â””â”€â”€ requirements.txt â”‚ â”œâ”€â”€ frontend/ â”‚ â”œâ”€â”€ index.html â”‚ â”œâ”€â”€ app.js â”‚ â”œâ”€â”€ style.css â”‚ â””â”€â”€ assets/ â”‚ â””â”€â”€ avatar.glb â”‚ â””â”€â”€ README.md

Prerequisites Required

Python 3.10+

Node.js (for local frontend serving)

Ollama

Install Ollama

Download and install Ollama from: ğŸ‘‰ https://ollama.com

Pull a model (example):

ollama pull phi3:mini

Ensure Ollama is running:

ollama serve

Backend Setup

Navigate to the backend directory:

cd backend

Create a virtual environment (recommended):

python -m venv venv source venv/bin/activate # macOS/Linux venv\Scripts\activate # Windows

Install dependencies:

pip install -r requirements.txt

Start the backend server:

uvicorn main:app --reload --port 8000

Backend will be available at:

http://localhost:8000

Frontend Setup

Navigate to the frontend directory:

cd frontend

Serve the frontend (any simple server works):

Using Python:

python -m http.server 5173

Or using Node:

npx serve .

Open your browser:

http://localhost:5173

How to Use 3D Asset Pipeline

Type a description (e.g. â€œhard hatâ€)

Click Generate

View the 3D model

Read the AI-generated educational explanation

Rotate and inspect the model

Avatar Commands

Switch to the Avatar tab (or section)

Type a command such as:

â€œWave helloâ€

â€œShow correct safety postureâ€

Watch the avatar animate

Read the AI explanation of the action

AI Logic (Ollama)

Ollama is used for:

Educational explanations

Natural language command interpretation

Prompts are intentionally constrained to:

Produce short, training-focused output

Return structured JSON for commands

This ensures deterministic and safe execution

Next Steps

Replace asset mapping with embedding-based search

Add text-to-3D or image-to-3D generation

Introduce spatial navigation for avatars

Build scenario-based simulations

Add analytics and learner feedback loops
